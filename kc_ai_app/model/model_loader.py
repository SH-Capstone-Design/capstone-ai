import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def load_model(device: str = None):
    """
    Hugging Face Hub ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ì„ ë¡œë“œí•¨.
    """
    # 1ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ ì§€ì •
    model_id = os.getenv("FINETUNED_MODEL_PATH", "JeongMin05/kcelectra-base-chat-emotion")

    # 2ï¸âƒ£ ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¹ Using device: {device}")
    print(f"ğŸ”¹ Loading model from: {model_id}")

    # 3ï¸âƒ£ Hugging Face ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForSequenceClassification.from_pretrained(model_id)

    model.to(device)
    model.eval()

    print("âœ… Model and tokenizer loaded successfully.")
    return tokenizer, model
